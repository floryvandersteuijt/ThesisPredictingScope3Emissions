---
title: "Regularization and Subset Selection for Linear Regression"

---


## Packages


```{r message = FALSE}
library("tidymodels")
library("glmnet")
library("leaps")
library("readr")
```

## Loading the data 


```{r}
linreg_recipe <- recipe(scope3 ~ ., data = data_final_train) %>% 
  step_normalize(all_predictors()) %>% 
  step_rm(number)
```


```{r}
data_final_train_baked <- linreg_recipe %>% prep(data_final_train) %>% bake(data_final_train)
head(data_final_train_baked)
```

```{r}
round(colMeans(data_final_train_baked), 8)
```

```{r}
round(apply(Credit_train_baked, 2, sd), 8)
```


## Ridge and lasso regression models


```{r}
ridge_linreg <- linear_reg(penalty = tune(), mixture = 0) %>% 
  set_engine("glmnet")
lasso_linreg <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```


## Combining the models and recipe into workflows

```{r}
ridge_wf <- workflow() %>% 
  add_recipe(linreg_recipe) %>% 
  add_model(ridge_linreg)
lasso_wf <- workflow() %>% 
  add_recipe(linreg_recipe) %>% 
  add_model(lasso_linreg)
```



## Tuning grids

```{r}
grid_lasso <- tibble(penalty = 5^(seq(from = -7, to = 0, length.out = 30)))
grid_ridge <- tibble(penalty = 5^(seq(from = -7, to = 0, length.out = 30)))
```

## Tuning lasso-penalized linear regression



```{r}
lasso_tune <- lasso_wf %>% 
  tune_grid(resamples = train_folds, # for cross-validation
            grid = grid_lasso,
            metrics = metric_set(rmse, rsq_trad, mae))
```


```{r}
lasso_tune_metrics <- lasso_tune %>% 
  collect_metrics()
lasso_tune_metrics %>% filter(.metric == "rmse") %>% 
  ggplot(aes(x = penalty, y = mean, 
             ymin = mean - std_err, ymax = mean + std_err)) + 
  geom_linerange(alpha = 0.5) + 
  geom_point() + 
  scale_x_log10()
```

```{r}
lasso_tune %>% collect_metrics()
```


```{r}
lasso_tune %>% show_best("mae")
```

```{r}
lasso_tune %>% show_best("rmse")
```
```{r}
lasso_tune %>% show_best("rsq_trad")
```


```{r}
lasso_best_model <- select_best(lasso_tune, metric = "mae", desc(penalty))
lasso_best_model
```



```{r}
lasso_wf_tuned <- 
  lasso_wf %>% 
  finalize_workflow(lasso_best_model)
lasso_wf_tuned
```

## Tuning ridge-penalized linear regression


```{r}
ridge_tune <- ridge_wf %>% 
  tune_grid(resamples = train_folds, 
            grid = grid_ridge,
            metrics = metric_set(rmse, rsq_trad, mae))
```


```{r}
ridge_tune_metrics <- ridge_tune %>% 
  collect_metrics()
ridge_tune_metrics %>% filter(.metric == "mae", penalty < 0.05) %>% 
  ggplot(aes(x=penalty, y=mean,
             ymin=mean -std_err, ymax=mean +std_err)) + 
  geom_errorbar(alpha =0.5) +
  geom_point()
```

```{r}
lasso_tune %>% collect_metrics()
```

```{r}
ridge_tune %>% show_best("mae")
```
```{r}
ridge_tune %>% show_best("rmse")
```
```{r}
ridge_tune %>% show_best("rsq_trad")
```



```{r}
ridge_best_model <- select_best(ridge_tune, metric = "rmse", desc(penalty))
ridge_best_model
```


```{r}
ridge_wf_tuned <- 
  ridge_wf %>% 
  finalize_workflow(ridge_best_model)
ridge_wf_tuned
```



## Selecting between lasso and ridge


```{r}
lasso_last_fit <- lasso_wf_tuned %>% 
  last_fit(data_final_splits, # for testing
           metrics = metric_set(rmse, mae, rsq_trad))
```



The performance on the test set for this model is:
```{r}
lasso_test_metrics <- lasso_last_fit %>% collect_metrics()
lasso_test_metrics
```



```{r}
ridge_last_fit <- ridge_wf_tuned %>% 
  last_fit(data_final_splits, # for testing
           metrics = metric_set(rmse, mae, rsq_trad))
ridge_test_metrics <- ridge_last_fit %>% collect_metrics()
ridge_test_metrics
```

